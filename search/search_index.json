{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#kit-i","title":"KIT-I","text":"<p>Luiza Teodoro de Abreu Coutinho</p> <p>Gabriel Pessine Buhrer</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"projeto/main/","title":"Relat\u00f3rio Completo do Projeto \u2013 API FastAPI + Scraper IBOVESPA","text":""},{"location":"projeto/main/#etapa-1-construcao-da-api","title":"Etapa 1 - Constru\u00e7\u00e3o da API","text":"<p>Nesta primeira etapa projeto criamos uma API em FastAPI que:</p> <ul> <li>Registra e autentica usu\u00e1rios via JWT.  </li> <li>Faz scraping do hist\u00f3rico de pre\u00e7os do \u00edndice IBOVESPA (Yahoo Finance).  </li> <li>Retorna os \u00faltimos 10 dias como JSON protegido por token.</li> </ul> <p>Para isso temos 3 endpoints:</p>"},{"location":"projeto/main/#1-postregistrar","title":"1. POST/registrar","text":"<ul> <li>Fun\u00e7\u00e3o: registrar um novo usu\u00e1rio.  </li> <li>Campos de entrada:  <ul> <li><code>nome</code> (string)  </li> <li><code>email</code> (string, formato email)  </li> <li><code>senha</code> (string) </li> </ul> </li> <li>Valida\u00e7\u00e3o: verifica no banco se o email j\u00e1 est\u00e1 cadastrado. </li> <li>Sa\u00edda: um JSON contendo o JWT (token).</li> </ul> <p>Body: <pre><code>{ \"nome\": \"Seu Nome\", \"email\": \"email@exemplo.com\", \"senha\": \"123456\" }\n</code></pre></p> <p>Resposta: <pre><code>{ \"jwt\": \"eyJ0eXAiOiJK...\" }\n</code></pre></p> <p>Segue esquema do endpoint POST/registrar</p> <pre><code>    sequenceDiagram\n        autonumber\n        actor Luiza\n        Luiza-&gt;&gt;+App: POST /registrar\n        App-&gt;&gt;+Postgres: consulta email\n        break se email encontrado\n            Postgres--&gt;&gt;Luiza: error 409\n        end\n        App-&gt;&gt;Postgres: grava dados e hash da senha no bd\n        App-&gt;&gt;App: gera JWT Token\n        App--&gt;&gt;-Luiza: retorna JWT Token</code></pre>"},{"location":"projeto/main/#2-postlogin","title":"2. POST/login","text":"<ul> <li>Fun\u00e7\u00e3o: autenticar usu\u00e1rio j\u00e1 registrado.</li> <li>Campos de entrada:  <ul> <li><code>email</code> (string, formato email)  </li> <li><code>senha</code> (string) </li> </ul> </li> <li>Valida\u00e7\u00e3o: Checa se email foi registrado e busca senha_hash no banco e compara com a senha informada.</li> <li>Sa\u00edda: um JSON contendo o JWT (token).</li> </ul> <p>Body: <pre><code>{ \"email\": \"email@exemplo.com\", \"senha\": \"123456\" }\n</code></pre></p> <p>Resposta: <pre><code>{ \"jwt\": \"eyJ0eXAiOiJK...\" }\n</code></pre></p> <p>Segue esquema do endpoint POST/login</p> <pre><code>    sequenceDiagram\n        autonumber\n        actor Luiza\n        Luiza-&gt;&gt;+App: POST /login\n        App-&gt;&gt;+Postgres: consulta email e hash no db\n        break se email n\u00e3o encontrado\n            Postgres--&gt;&gt;Luiza: error 401\n        end\n        break se email e senha n\u00e3o confere\n            Postgres--&gt;&gt;Luiza: error 401\n        end\n        App-&gt;&gt;App: gera JWT Token\n        App--&gt;&gt;-Luiza: retorna JWT Token</code></pre>"},{"location":"projeto/main/#3-getconsultar","title":"3. GET/consultar","text":"<ul> <li>Fun\u00e7\u00e3o: autenticar usu\u00e1rio j\u00e1 registrado.</li> <li> <p>Autoriza\u00e7\u00e3o: precisa do header  <pre><code>Authorization: Bearer &lt;seu_jwt_aqui&gt;\n</code></pre></p> <ul> <li>No cadeado acima do \"Try it out\" \u00e9 necess\u00e1rio colocar o token e autorizar, depois \u00e9 s\u00f3 rodar o execute!!</li> </ul> </li> <li> <p>Processo:</p> <ol> <li> <p>Decodifica o JWT e valida o token.</p> </li> <li> <p>Faz scraping da p\u00e1gina do Yahoo Finance.</p> </li> <li> <p>Extrai data, abertura, m\u00e1xima, m\u00ednima, fechamento e volume.</p> </li> </ol> </li> <li> <p>Sa\u00edda: JSON com a lista de 10 dias do IBOVESPA.</p> </li> </ul> <p>Resposta: <pre><code>{\"ultimos_10_dias\": [ { \"data\":\"...\", \"abertura\":\"...\", \u2026 }, \u2026 ]}\n</code></pre></p> <p>Segue esquema do endpoint GET/consultar</p> <pre><code>    sequenceDiagram\n        autonumber\n        actor Luiza\n        Luiza-&gt;&gt;+App: GET /consultar &lt;br&gt; Token JWT no header\n        App--&gt;&gt;App: verifica permiss\u00e3o do JWT\n        break se JWT ausente ou inv\u00e1lido\n            App--&gt;&gt;Luiza: error 403\n        end\n        App--&gt;&gt;App: web scraping&lt;br&gt;solicita dados de uma base ou p\u00e1gina&lt;br&gt;de um 3th party\n        Note right of App: Adquire dados da internet, &lt;br&gt;fazendo scraping de quaisquer&lt;br&gt; dados interessantes para o aluno.&lt;br&gt;O conte\u00fado deve ter atualiza\u00e7\u00e3o frequente.\n        App--&gt;&gt;-Luiza: retorna dados</code></pre>"},{"location":"projeto/main/#explicacao-do-passo-a-passo","title":"Explica\u00e7\u00e3o do passo a passo","text":"<ol> <li> <p>Primeiro formatamos o reposit\u00f3rio e todos os arquivos.</p> </li> <li> <p>Definimos os models: UserCreate e UserLogin </p> </li> <li> <p>Seguran\u00e7a e Autentica\u00e7\u00e3o : Garantimos que quando o usu\u00e1rio colocasse sua senha que ela fosse transformada em um hash.</p> </li> <li> <p>Extra\u00e7\u00e3o de Dados \u2013 Scraper IBOVESPA  </p> <ul> <li>Requisi\u00e7\u00f5es HTTP: usamos <code>requests</code> com <code>User-Agent</code> padr\u00e3o.</li> <li>Parsing com BeautifulSoup: localizamos o <code>&lt;tbody&gt;</code> da tabela hist\u00f3rica via seletor CSS (Inspect) do site do Yahoo Finance e extra\u00edmos data, abertura, m\u00e1xima, m\u00ednima, fechamento e volume para os 10 primeiros registros do IBOVESPA.  </li> </ul> </li> <li> <p>Dockeriza\u00e7\u00e3o e Orquestra\u00e7\u00e3o  </p> <ul> <li>Dockerfile: configuramos imagem leve (Python slim), instalamos depend\u00eancias e expomos a aplica\u00e7\u00e3o na porta 8080.  </li> </ul> </li> </ol>"},{"location":"projeto/main/#teste-dos-endpoints","title":"Teste dos ENDPOINTS","text":"<p>POST/registrar</p> <p></p> <p>POST/login</p> <p></p> <p>GET/consultar</p>"},{"location":"projeto/main/#link-para-video","title":"Link para V\u00eddeo","text":"<p>https://youtu.be/Dt5RYUkA8l8</p>"},{"location":"projeto/main/#link-para-docker-hub","title":"Link para docker hub","text":"<p>https://hub.docker.com/repository/docker/luizatac/projeto_cloudi-app/general</p>"},{"location":"projeto/main/#mapa-do-repositorio","title":"Mapa do reposit\u00f3rio","text":"<pre><code>projeto-cloudi-app/\n\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 app/              \n\u2502   \u2502   \u2514\u2500\u2500 app.py\n\u2502   \u251c\u2500\u2500 .env              \n\u2502   \u251c\u2500\u2500 Dockerfile         \n\u2502   \u2514\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 compose.yaml           \u27f5 arquivo compose.yaml\n\u251c\u2500\u2500 .gitignore\n\u2514\u2500\u2500 README.md \n</code></pre> <p>Segue abaixo arquivo compose.yaml FINAL do projeto:</p> compose.yaml<pre><code>    services:\n  app:\n    image: luizatac/projeto_cloudi-app:cloudi\n    container_name: app\n    env_file: .env\n    environment:\n      - POSTGRES_DB=${POSTGRES_DB}\n      - POSTGRES_USER=${POSTGRES_USER}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n      - JWT_SECRET_KEY=${JWT_SECRET_KEY}\n      - DB_HOST=database\n    ports:\n      - \"8080:8080\"\n    depends_on:\n      - database\n\n  database:\n    image: postgres:17\n    container_name: database\n    env_file: .env                 \n    environment:\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: ${POSTGRES_DB}\n    ports:\n      - \"5432:5432\"\n</code></pre>"},{"location":"roteiro1/main/","title":"Roteiro 1","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>O objetivo geral deste primeiro roteiro \u00e9 compreender os conceitos fundamentais de uma plataforma de gerenciamento de hardware, especificamente o MAAS (Metal as a Service), e introduzir no\u00e7\u00f5es b\u00e1sicas sobre redes de computadores, proporcionando uma base s\u00f3lida para futuro aprofundamento.</p> <p>Para isso, foi utilizado o seguinte material:</p> <pre><code>1 NUC (main) com 10Gb e 1 SSD (120Gb)\n1 NUC (server1) com 12Gb e 1 SSD (120Gb)\n1 NUC (server2) com 16Gb e 2 SSD (120Gb+120Gb)\n3 NUCs (server3, server4 e server5) com 32Gb e 2 SSD (120Gb+120Gb)\n1 Switch DLink DSG-1210-28 de 28 portas\n1 Roteador TP-Link TL-R470T+\n</code></pre>"},{"location":"roteiro1/main/#criando-a-infraestrutura","title":"Criando a Infraestrutura","text":""},{"location":"roteiro1/main/#instalacao-do-ubuntu","title":"Instala\u00e7\u00e3o do Ubuntu","text":"<p>Realizou-se a instala\u00e7\u00e3o do Sistema Operacional Ubuntu Server 22.04 LTS na NUC main. Para isso, a imagem do mesmo foi baixada, colocada em um pendrive e, esse, utilizado para realizar o boot de instala\u00e7\u00e3o na m\u00e1quina. Com isso, definiu-se o hostname, o login, a senha, o IP fixo (172.16.0.3) e o DNS (172.20.129.131).</p>"},{"location":"roteiro1/main/#acesso-a-main","title":"Acesso \u00e0 main","text":"<p>Para acessar a m\u00e1quina main, conectou-se um cabo de rede entre o computador e o switch, permitindo o acesso \u00e0 rede local.</p> <p>Dentro da rede, utilizou-se o seguinte comando:</p> ssh cloud@172.16.0.3 <p>Como pode ser observado na figura abaixo:</p> <p></p> <p>Acesso \u00e0 m\u00e1quina main</p>"},{"location":"roteiro1/main/#instalacao-do-maas","title":"Instala\u00e7\u00e3o do MAAS","text":"<p>O MAAS foi instalado para atuar como o gerenciador de hardware do projeto.</p> <p>Para realizar sua instala\u00e7\u00e3o, na vers\u00e3o \"stable\" 3.5, dentro da m\u00e1quina main, utilizaram-se os seguintes comandos:</p> <p>I.</p> sudo apt update &amp;&amp; sudo apt upgrade -y <p>II.</p> sudo snap install maas --channel=3.5/stable <p>III.</p> sudo snap install maas-test-db"},{"location":"roteiro1/main/#configuracao-do-maas","title":"Configura\u00e7\u00e3o do MAAS","text":"<p>Primeiramente, foi realizada a inicializa\u00e7\u00e3o do MAAS, da seguinte maneira:</p> sudo maas init region+rack --maas-url http://172.16.0.3:5240/MAAS --database-uri maas-test-db:/// <p>Em seguida, a cria\u00e7\u00e3o do usu\u00e1rio admin:</p> sudo maas createadmin <p>Com isso, determinou-se o login e a senha.</p> <p>Posteriormente, foi necess\u00e1rio gerar um par de chaves para autentica\u00e7\u00e3o, utilizando o seguinte comando:</p> ssh-keygen -t rsa <p>E acessar essa chave para poder copi\u00e1-la, dessa maneira:</p> cat ./.ssh/id_rsa.pub <p>A chave p\u00fablica obtida foi a da figura a seguir:</p> <p></p> <p>Chave p\u00fablica</p>"},{"location":"roteiro1/main/#acessando-o-dashboard-do-maas","title":"Acessando o dashboard do MAAS","text":"<p>O dashboard do MAAS foi acessado atrav\u00e9s da seguinte URL:</p> <pre><code>http://172.16.0.3:5240/MAAS\n</code></pre> <p>Por\u00e9m, antes disso, foi necess\u00e1rio desativar o UFW(firewall), pois ele estava bloqueando todas as portas, menos a 22.</p> <p>Ap\u00f3s realizar o login, configurou-se o DNS Forwader com o DNS do Insper, sendo assim, quando um dispositivo realizar uma consulta DNS, essa solicita\u00e7\u00e3o ser\u00e1 encaminhada para o DNS do Insper.</p> <p>Em seguida, as imagens do Ubuntu 22.04LTS e Ubuntu 20.04LTS foram importadas.</p> <p></p> <p>Imagens Ubuntu</p> <p>Observa\u00e7\u00e3o: em momento posterior, essas imagens foram corrompidas por uma atualiza\u00e7\u00e3o da pr\u00f3pria Cannonical, sendo assim, foi utilizada a imagem do Ubuntu 24.04 LTS para a realiza\u00e7\u00e3o de tarefas que est\u00e3o mais \u00e0 frente</p> <p>Por fim, foi realizado o upload da chave SSH gerada, permitindo acesso ao servidor sem pecisar de senha.</p>"},{"location":"roteiro1/main/#chaveando-o-dhcp","title":"Chaveando o DHCP","text":"<p>O passo seguinte foi habilitar o DHCP no MAAS, permitindo o gerenciamento dos IPs das outras m\u00e1quinas.</p> <p>Para isso, acessou-se o MAAS Controller e definiu-se o reserved range para iniciar em 172.16.11.1 e acabar em 172.16.14.255. Al\u00e9m disso, o DNS da subnet passou a apontar para o DNS do Insper e, por fim, desabilitou-se o DHCP no roteador.</p> <p></p> <p>DHCP habilitado no MAAS</p>"},{"location":"roteiro1/main/#checando-a-saude-do-maas","title":"Checando a sa\u00fade do MAAS","text":"<p>Com o intuito de garantir que estava tudo correto com o MAAS, verificou-se o estado dos servi\u00e7os do sistema na aba de controladores. A figura abaixo demonstra que tudo ocorreu com \u00eaxito.</p> <p></p> <p>Sa\u00fade do MAAS</p>"},{"location":"roteiro1/main/#comissionando-servidores","title":"Comissionando servidores","text":"<p>Ap\u00f3s verificar o funcionamento do MAAS, os servidores foram comissionados, ou seja, as m\u00e1quinas, do server1 ao server5, foram cadastradas no MAAS.</p> <p>Primeiramente, alterou-se a op\u00e7\u00e3o \"power type\" para INtel AMT, pois essa \u00e9 a tecnologia de gerenciamento remoto que est\u00e1 presente nas m\u00e1quinas. Ap\u00f3s isso, foram preenchidas as informa\u00e7\u00f5es sobre o MacAdress das m\u00e1quinas, presente na parte inferior das mesmas, a senha e o IP do AMT e, como pode-se observar na figura abaixo, o processo ocorreu da maneira desejada.</p> <p></p> <p>M\u00e1quinas comissionadas</p> <p>O roteador tamb\u00e9m foi adicionado como device no dashboard do MAAS.</p> <p></p> <p>Router adicionado</p>"},{"location":"roteiro1/main/#criando-ovs-bridge","title":"Criando OVS bridge","text":"<p>A OVS tem o objetivo de permitir conectar mais de uma interface de rede dentro de um servidor f\u00edsico. Para cri\u00e1-las, acessou-se a aba rede no MAAS e criou-se uma ponte baseada na interface regular \"enp1s0\".</p>"},{"location":"roteiro1/main/#fazendo-acesso-remoto-ao-kit","title":"Fazendo acesso remoto ao Kit","text":"<p>O objetivo dessa parte do roteiro \u00e9 conseguir conectar o server main utilizando a porta 22, via SSH. Para isso, realizou-se um NAT para garantir o acesso externo \"Rede Wi-fi Insper\" ao main.</p> <p>Primeiramente, foi necess\u00e1rio abrir a porta 22. Para tal, acessou-se a p\u00e1gina do roteador, na aba transmition e depois na aba NAT. Com isso, definiu-se a rede wan1, colocando o ip do roteador (10.103.1.18), a porta externa e interna como 22, o ip interno (172.16.0.3) e o nome como ssh.</p> <p></p> <p>Configurando NAT</p> <p>Feito isso, para acessar a main bastou utilizar o seguinte comando:</p> ssh cloud@10.103.1.18 <p></p> <p>Acessando a main</p> <p>Ademais, realizou-se a libera\u00e7\u00e3o do acesso ao gerenciamento remoto do roteador, criando uma regra de gest\u00e3o para a rede 0.0.0.0/0, liberando acesso para todas as m\u00e1quinas que est\u00e3o no WI-fi Insper.</p> <p></p> <p>Cria\u00e7\u00e3o da regra de gest\u00e3o</p>"},{"location":"roteiro1/main/#app","title":"APP","text":""},{"location":"roteiro1/main/#instalando-maas","title":"Instalando MAAS","text":"sudo snap install maas --channel=3.5/Stable <p>Dashboard do MAAS</p> <p>Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado. Os pain\u00e9is podem ser configurados e personalizados de acordo com as necessidades do usu\u00e1rio.</p> <p>Para iniciar nossa aplica\u00e7\u00e3o iniciamos adicionando o DNS do Insper (172.20.129.131) seguindo o padr\u00e3o. Com isso conseguimos acessar nossa m\u00e1quina facilmente dentro da rede do Insper.</p>"},{"location":"roteiro1/main/#primeira-parte-banco-de-dados","title":"Primeira Parte - Banco de Dados","text":"<p>A primeira etapa da Tarefa 1 consistiu na cria\u00e7\u00e3o de um banco de dados PostgreSQL no server 1. Para isso, foi necess\u00e1rio implantar o Ubuntu 22.04 para Linux no servidor por meio do dashboard do MAAS, uma vez que ele ainda n\u00e3o possu\u00eda um sistema operacional.</p> <p></p> <p>Tela do MAAS exemplificando processo de dar deploy do Ubuntu 22.04 no Server 1</p> <p>Finalizado o Deploy, acessamos o server 1 pelo terminal e criamos o banco manualmente utilizando os comandos abaixo:</p> sudo apt updatesudo apt install postgresql postgresql-contrib -y <p>Com o PostgreSQL instalado, criamos um usu\u00e1rio e um database para a aplica\u00e7\u00e3o. Por\u00e9m, para liberar o acesso remoto, tivemos que configurar o arquivo de configura\u00e7\u00e3o:</p> <p><pre><code>  nano /etc/postgresql/14/main/postgresql.conf\n</code></pre> Adicionando a seguinte linha:</p> <pre><code>  listen_addresses = '*'\n</code></pre> <p>O \u00faltimo passo foi liberar o acesso para qualquer m\u00e1quina dentro da subnet do kit com o seguinte comando:</p> <p><code>host    all             all             172.16.0.0/20          trust</code></p> <p>E, por fim, liberamos o firewall:</p> <pre><code>  sudo ufw allow 5432/tcp\n</code></pre>"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Nossa primeira atividade foi verificar se o banco de dados PostgreSQL estava corretamente implantado e funcionando. Para isso, precisamos confirmar que o servi\u00e7o estava ativo no sistema operacional, acess\u00edvel localmente na m\u00e1quina onde foi instalado e que tamb\u00e9m podia ser alcan\u00e7ado a partir da m\u00e1quina MAIN, al\u00e9m de identificar em qual porta o servi\u00e7o estava operando.</p> <p>Para nos ajudar, utilizamos alguns comandos como: ping, ifconfig, systemctl, telnet, ufw, curl, wget e journalctl</p> <p>I.</p> systemctl status postgresql <p></p> <p>Banco funcionando e seu Status est\u00e1 como \"Ativo\" para o Sistema Operacional</p> <p>II.</p> psql -U cloud -h 172.16.15.0 tasks <p></p> <p>Banco acess\u00edvel na pr\u00f3pria maquina na qual ele foi implantado.</p> <p>III.</p> psql -U cloud -h 172.16.15.0 tasks <p></p> <p>Banco acess\u00edvel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN</p> <p>IV.</p> nmap localhost <p></p> <p>Porta este servi\u00e7o est\u00e1 funcionando</p>"},{"location":"roteiro1/main/#segunda-parte-aplicacao-django","title":"Segunda Parte - Aplica\u00e7\u00e3o Django","text":"<p>A segunda parte do roteiro foi adicionar uma Aplica\u00e7\u00e3o Django na nossa m\u00e1quina, mais especificamente no server 2.</p> <p>Por\u00e9m, at\u00e9 ent\u00e3o s\u00f3 temos o server 2 fisicamente, precisamos criar ele e adicionar o sistema operacional. Para esse processo decidimos adicionar o server 2 usando o terminal, mas esse processo tamb\u00e9m pode ser feito no Dashboard do MAAS.</p> <ul> <li>Primeiro pedimos uma m\u00e1quina</li> </ul>   maas login cloud http://172.16.0.3:5240/MAAS/ <ul> <li> <p>Para evitar ter que sempre autenticar e dar a senha em todos os passos, procuramos a nossa API-KEYS na aba API-KEYS do MAAS.</p> </li> <li> <p>Seguimos solicitando a reserva do server 2:</p> </li> </ul>   maas cloud machines allocate name=server2 <ul> <li>Como fizemos pelo Dashboard o deploy do Ubuntu 22.04 no server1, conseguimos fazer por linha de c\u00f3digo para o server 2:</li> </ul> maas cloud machine deploy [system_id] <ul> <li> <p>Esse system_id vem do comando de reserva de m\u00e1quina!!</p> </li> <li> <p>Agora entramos no server 2 utilizando:   <code>ssh ubuntu@172.16.0.7</code> , que conseguimos ver diretamente da p\u00e1gina machines no MAAS.</p> </li> <li> <p>No ssh do server 2 vamos realizar a instala\u00e7\u00e3o do arquivo tasks.</p> </li> </ul> git clone https://github.com/raulikeda/tasks.git ./install.sh <ul> <li>O pr\u00f3ximo passo foi fazer um GET para acessar a URL e abrir visualizar a interface do Django.</li> </ul> wget http://[IP server2]:8080/admin/ <ul> <li>Mesmo conseguindo acessar o server 2 de dentro, podemos facilitar a entrada no server 2, ou seja, a entrada na interface do Django. Para acessar isso no browser seria necess\u00e1rio fazer um NAT no roteador. Por\u00e9m vamos criar  um TUNEL SSH, e com esse servi\u00e7o tempor\u00e1rio podemos expor o servi\u00e7o para fora do kit enquanto o terminal que o tunnel estiver utilizando esteja ativo.</li> </ul> <p>A ideia principal vai fazer com que ap\u00f3s acessar o main pela porta 22, se eu digitasse localhost[alguma porta] , eu iria parar direto no server 2.</p> <p>Primeiro sa\u00edmos da nossa m\u00e1quina e no terminal da nossa m\u00e1quina f\u00edsica digitamos o seguinte comando:</p> <pre><code>ssh cloud@10.103.0.X -L 8001:[IP server2]:8080\n</code></pre> <ul> <li>Mudamos a linha de c\u00f3digo acima para se adequar a nossa m\u00e1quina, mudando o IP do roteador e o IP do server 2.</li> </ul> ssh cloud@10.103.1.18 -L 8001:172.16.0.7:8080"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":"<p>T\u00ednhamos as seguintes perguntas para a tarefa 2:</p> <ol> <li>Do Dashboard do MAAS com as m\u00e1quinas.</li> <li>Da aba images, com as imagens sincronizadas.</li> <li>Da Aba de cada maquina(5x) mostrando os testes de hardware e commissioning com Status \"OK\"</li> </ol> <p>I.</p> <p></p> <p>Machines no MAAS</p> <p>II.</p> <p></p> <p>Imagens sincronizadas</p> <p>III.</p> <p></p> <p>Server 1</p> <p></p> <p>Server 2</p> <p></p> <p>Server 3</p> <p></p> <p>Server 4</p> <p></p> <p>Server 5</p>"},{"location":"roteiro1/main/#utilizando-o-ansible-deploy-automatizado-de-aplicacao","title":"Utilizando o Ansible - deploy automatizado de aplica\u00e7\u00e3o","text":""},{"location":"roteiro1/main/#tarefa-3","title":"Tarefa 3","text":"<ol> <li>De um print da tela do Dashboard do MAAS com as 2 Maquinas e seus respectivos IPs.</li> <li>De um print da aplicacao Django, provando que voce est\u00e1 conectado ao server</li> <li>Explique como foi feita a implementacao manual da aplicacao Django e banco de dados.</li> </ol> <p>I.</p> <p></p> <p>Imagens sincronizadas</p> <p>II.</p> <p></p> <p>Server 1</p> <p>III. Para realizar essas a\u00e7\u00f5es manualmente, seguimos o roteiro do professor. Revisitar:</p> <ul> <li> <p>Primeira parte - Banco de Dados </p> </li> <li> <p>Segunda parte - Aplica\u00e7\u00e3o Django</p> </li> </ul> <p>At\u00e9 agora tinhamos uma Aplica\u00e7\u00e3o Django, mas agora teremos 2 aplica\u00e7\u00f5es django (server2 e server3) compartilhando o mesmo banco de dados (server1). Os motivos para fazermos isso s\u00e3o dois:</p> <ul> <li> <p>Alta disponibilidade: se um node cair o outro est\u00e1 no ar, para que nosso cliente acesse.</p> </li> <li> <p>Load Balancing: podemos dividir a carga de acesso entre os n\u00f3s.</p> </li> </ul> <p>Para fazer essa aplica\u00e7\u00e3o fizemos tudo manualmente a partir do terminal, mas existem aplica\u00e7\u00f5es que realizam esse trabalho autom\u00e1ticamente para n\u00f3s. Nesse roteiro vamos usar o gerenciador de deploy Ansible</p> <ul> <li> <p>Para come\u00e7ar pedimos  o deploy no server 3 para o MAAS via cli</p> </li> <li> <p>Depois dentro do MAIN fizemos a instala\u00e7\u00e3o do Ansible, usando o seguinte comando:</p> </li> </ul>  sudo apt install ansible <ul> <li></li> </ul> wget https://raw.githubusercontent.com/raulikeda/tasks/master/tasks-install-playbook.yaml <ul> <li></li> </ul> ansible-playbook tasks-install-playbook.yaml --extra-vars server=172.16.0.10"},{"location":"roteiro1/main/#tarefa-4","title":"Tarefa 4","text":"<ol> <li>De um print da tela do Dashboard do MAAS com as 3 Maquinas e seus respectivos IPs.</li> <li>De um print da aplicacao Django, provando que voce est\u00e1 conectado ao server2 </li> <li>De um print da aplicacao Django, provando que voce est\u00e1 conectado ao server3 </li> <li>Explique qual diferenca entre instalar manualmente a aplicacao Django e utilizando o Ansible.</li> </ol> <p>I.</p> <p></p> <p>Machines no MAAS</p> <p>II.</p> <p></p> <p>Server 2</p> <p>III.</p> <p></p> <p>Server 3</p> <p>IV. Primeiro, fizemos manualmente o deploy do server 2 via MAAS e instalamos o Django pelo terminal. Para acess\u00e1-lo, entramos na main via SSH (porta 22) e depois no server 2 pela porta 8080.</p> <p>Para automatizar a instala\u00e7\u00e3o, usamos o Ansible, que executa tudo de uma vez com um playbook (script com todos os comando compactados). Al\u00e9m disso, vamos criar um t\u00fanel SSH para acessar o server 2 diretamente pela porta 8001, sem precisar passar pela main manualmente.</p>"},{"location":"roteiro1/main/#balancamento-de-carga-usando-proxy-reverso","title":"Balancamento de carga usando Proxy Reverso","text":"<p>Para montar o ponto \u00fanico de entrada, utilizaremos uma aplica\u00e7\u00e3o de proxy reverso como load balancer. Vamos usar o server 4 e usar o NGINX nele.</p> <p>NGINX</p> <p>Loadbalancing \u00e9 um mecanismo \u00fatil para distribuir o tr\u00e1fego de entrada por v\u00e1rios servidores privados virtuais capazes.</p> <ul> <li>Em resumo, ele vai fazer o balanceamento das m\u00e1quinas (server 2 ou 3).</li> </ul> <p></p> <p>Resultado final que queremos</p> <ul> <li>Primeiro instalamos o NGINX no server 4</li> </ul> <pre><code>sudo apt-get install nginx\n</code></pre> <ul> <li>Para configurar um loadbalancer round robin, precisamos usar o m\u00f3dulo upstream do nginx. Incorporamos a configura\u00e7\u00e3o nas defini\u00e7\u00f5es do nginx.</li> </ul> <pre><code>sudo nano /etc/nginx/sites-available/default\n</code></pre> <ul> <li>Precisamos incluir o m\u00f3dulo upstream, que se parece com isto:</li> </ul> <pre><code>upstream backend {server 172.16.0.7; server 172.16.0.10; }\nserver { location / { proxy_pass http://backend; } }\n</code></pre> <ul> <li>Depois reiniciamos o NGINX</li> </ul> <pre><code>sudo service nginx restart\n</code></pre> <ul> <li> <p>Agora para identificar cada server, precisamos alterar no arquivo de tasks/views.py de cada Django a mensagem \"Hello World ...\".</p> </li> <li> <p>Entrando no server 2 e 3 de cada vez digitamos em cada:</p> </li> </ul> <pre><code>cd tasks/tasks/views.py\n</code></pre> <pre><code>from django.shortcuts import render\n\nfrom django.http import HttpResponse\n\ndef index(request):\n\n  return HttpResponse(\"Server X\")\n</code></pre> <ul> <li> <p>Mudamos o \"X\" para o n\u00famero do Server que est\u00e1vamos !!</p> </li> <li> <p>No arquivo de urls.py de cada Server customizamos o path no urlpatterns:</p> </li> </ul> <pre><code>cd tasks/tasks/urls.py\n</code></pre> <pre><code>urlpatterns = [path( ' ', views.index, name='index'),]\n</code></pre>"},{"location":"roteiro1/main/#tarefa-5","title":"Tarefa 5","text":"<ol> <li>De um print da tela do Dashboard do MAAS com as 4 Maquinas e seus respectivos IPs.</li> <li>Altere o conte\u00fado da mensagem contida na fun\u00e7\u00e3o <code>index</code> do arquivo <code>tasks/views.py</code> de cada server para distinguir ambos os servers. </li> <li>Fa\u00e7a um <code>GET request</code> para o path que voce criou em urls.py para o Nginx e tire 2 prints das respostas de cada request, provando que voce est\u00e1 conectado ao server 4, que \u00e9 o Proxy Reverso e que ele bate cada vez em um server diferente server2 e server3.</li> </ol> <p>I.</p> <p></p> <p>Machines no MAAS</p> <p>II.</p> <p></p> <p>Altera\u00e7\u00e3o URL do Server 2</p> <p></p> <p>Altera\u00e7\u00e3o URL do Server 3</p> <p>III.</p> <p></p> <p>C\u00f3digo para GET Request</p> <p></p> <p>GET Request Server 2</p> <p></p> <p>GET Request Server 3</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Acreditamos que as maiores dificuldades foram problemas relacionados a vers\u00f5es de aplica\u00e7\u00f5es de terceiros que, em alguns casos, haviam sido atualizadas, exigindo que busc\u00e1ssemos novas documenta\u00e7\u00f5es para entender a maneira correta de utiliz\u00e1-las para o que precis\u00e1vamos. Al\u00e9m disso, o entendimento da teoria de computa\u00e7\u00e3o em nuvem foi desafiador, pois, muitas vezes, s\u00e3o t\u00f3picos complexos e que, sem domin\u00e1-los, n\u00e3o poder\u00edamos prosseguir para as pr\u00f3ximas etapas. Por outro lado, quando compreendidos, as coisas passavam a fazer mais sentido, tornando o processo mais natural.</p> <p>Depois de fazermos algumas a\u00e7\u00f5es manualmente e fixarmos bem os conceitos, quando avan\u00e7amos nos roteiros e vimos aplica\u00e7\u00f5es que automatizavam essas a\u00e7\u00f5es, como o MAAS, Ansible e Nginx, nosso entendimento e aplica\u00e7\u00e3o dos processos tornou-se mais claro e produtivo.</p>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Por meio deste roteiro, pudemos compreender a complexidade das redes de computadores, bem como sua import\u00e2ncia para projetos que demandam escalabilidade e efici\u00eancia. A explora\u00e7\u00e3o de ferramentas como o MAAS (Metal as a Service), o Ansible e o Nginx nos permitiu compreender conceitos sobre gerenciamento de hardware, automa\u00e7\u00e3o de tarefas e redes, destacando a relev\u00e2ncia de aplica\u00e7\u00f5es robustas nesse contexto. Essa base te\u00f3rica e pr\u00e1tica esclareceu aspectos essenciais da computa\u00e7\u00e3o em nuvem e abre caminho para futuros aprofundamentos e aplica\u00e7\u00f5es em projetos mais complexos.</p>"},{"location":"roteiro2/main/","title":"Roteiro 2","text":""},{"location":"roteiro2/main/#objetivo","title":"Objetivo","text":"<p>O objetivo deste segundo roteiro \u00e9 compreender os conceitos essenciais de uma plataforma de gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas, nesse caso, o Juju da Canonical, explorando orquestra\u00e7\u00e3o, escalabilidade e monitoramento. Al\u00e9m disso, compreender os fundamentos de comunica\u00e7\u00e3o entre aplica\u00e7\u00f5es e servi\u00e7os.</p>"},{"location":"roteiro2/main/#criando-a-infraestrutura-para-deploy-com-juju","title":"Criando a Infraestrutura para deploy com Juju","text":"<p>No roteiro anterior, utilizou-se o Ansible como gerenciador de deploy, lidando com a instala\u00e7\u00e3o e configura\u00e7\u00e3o de um conjunto de n\u00f3s, por\u00e9m, ele n\u00e3o consegue realizar o provisionamento direto com o MAAS. Sendo assim, nesse roteiro, ser\u00e1 utilizado o Juju.</p> <p>Primeiramente, realizou-se o release de todas as m\u00e1quinas no MAAS (server1 ao server5). Com isso feito, acessou-se a m\u00e1quina main, via SSH, e foi realizada a instala\u00e7\u00e3o do Juju, com o seguinte comando.</p> sudo snap install juju --channel 3.6 <p>Em seguida, foi realizada a verifica\u00e7\u00e3o se o Juju estava enxergando o MAAS como um provedor de recursos, utilizando o seguinte comando.</p> juju clouds <p>Assim como demonstra a figura a seguir, o MAAS n\u00e3o estava como provider do Juju.</p> <p></p> <p>Para resolver essa quest\u00e3o, come\u00e7ou-se adicionando o cluster MAAS para que o Juju pudesse. gerenci\u00e1-lo como uma cloud. Desse modo, o Juju utilizaria o MAAS como provedor de m\u00e1quinas e SO (Sistema Operacional).</p> <p>Sendo assim, criou-se um arquivo de defini\u00e7\u00e3o de cloud, o \"maas-cloud.yaml\", adicionando o ip da m\u00e1quina main (172.16.0.3) e a porta do MAAS (5240) no endpoint, como apresentado na figura abaixo.</p> <p></p> <p>maas-cloud.yaml</p> <p>Ap\u00f3s isso, adicionou-se a nova cloud com o seguinte comando.</p> juju add-cloud --client -f maas-cloud.yaml maas-one <p>E o resultado est\u00e1 demonstrado na figura abaixo.</p> <p></p> <p>Por\u00e9m, foi necess\u00e1rio, ap\u00f3s a cria\u00e7\u00e3o, adicionar as credenciais MAAS, permitindo a intera\u00e7\u00e3o do Juju com a nova cloud. Para tal, criou-se o arquivo \"maas-creds.yaml\" para importar as informa\u00e7\u00f5es necess\u00e1rias, como demonstrado abaixo.</p> <p></p> <p>maas-creds.yaml</p> <p>Observa\u00e7\u00e3o: O campo maas-oauth recebeu o API key gerado no MAAS e que est\u00e1 localizado dentro do menu de usu\u00e1rio.</p> <p>Em seguida, utilizou-se o seguinte comando para adicionar a credencial.</p> juju add-credential --client -f maas-creds.yaml maas-one <p>Como mostrado na figura abaixo.</p> <p></p> <p>Adicionando a credencial</p> <p>O passo seguinte foi criar o controlador para a cloud rec\u00e9m-criada (maas-one). Ele foi criado no server1 utilizando a s\u00e9rie \"jammy\" do Ubuntu.</p> <p>Para que isso fosse poss\u00edvel, fez-se a tag da m\u00e1quina com o nome \"juju\" no dashboard do MAAS (aba machines -&gt; categorize -&gt; new tag).</p> <p></p> <p>Tag \"juju\" no server1</p> <p>Tendo isso feito, o comando a seguir foi rodado.</p> juju bootstrap --bootstrap-series=jammy --constraints tags=juju maas-one maas-controller <p>Com isso, o Juju e o Ubuntu foram instalados no server1, como \u00e9 poss\u00edvel observar na figura abaixo.</p> <p></p> <p>Server1 \"deployed\"</p> <p>Por fim, criou-se um novo modelo, determinando qual o controlador que deveria ser instalado pelo Juju, nesse caso, o openstack.</p> <p>Para isso, utilizou-se o comando abaixo.</p> juju add-model --config default-series=jammy openstack <p></p> <p>Modelo criado</p> <p>Ap\u00f3s isso, verificou-se o status do controller com o seguinte comando.</p> juju status <p></p> <p>Controller status</p> <p>Todo esse processo foi respons\u00e1vel por alocar o server1 como o controlador de deploy para o Juju.</p>"},{"location":"roteiro2/main/#app","title":"APP","text":"<p>Antes da realiza\u00e7\u00e3o do deploy de instala\u00e7\u00f5es com o Juju, realizou-se a instala\u00e7\u00e3o do dashboard do Juju.</p> <p>O Juju possui um modelo pr\u00f3prio de gerenciador chamado controller, que \u00e9 respons\u00e1vel por gerenciar a pr\u00f3pria aplica\u00e7\u00e3o em si.</p> <p>Primeiramente, foi necess\u00e1rio mudar para esse modelo, usando o comando a seguir.</p> juju switch controller <p>Ap\u00f3s isso, criou-se uma m\u00e1quina virtual no server1, pois n\u00e3o foi poss\u00edvel alocar uma m\u00e1quina para o dashboard, e deu-se deploy na aplica\u00e7\u00e3om utilizando o seguinte comando.</p> juju deploy juju-dashboard --to lxd:0 <p></p> <p>Deploy do dashboard na m\u00e1quina virtual</p> <p>Uso do Juju status para verificar se deu certo.</p> <p></p> <p>Feito isso, criou-se um t\u00fanel para acessar a aplica\u00e7\u00e3o na m\u00e1quina virtual, utlizando o comando a seguir.</p> ssh cloud@10.103.1.18 -L 8081:172.16.0.19:8080 <p>E conseguiu-se acessar o dashboard.</p> <p></p> <p>Acesso ao dashboard</p> <p>Ap\u00f3s o deploy e instala\u00e7\u00e3o do dashboard, foram feitas as instala\u00e7\u00f5es do Grafana e do Prometheus. O Grafana \u00e9 uma plataforma de c\u00f3digo aberto para visualiza\u00e7\u00e3o de dados, facilitando a an\u00e1lise em tempo real. Ele requer um banco de dados para armazenar configura\u00e7\u00f5es e metadados, neste caso, decidiu-se utilizar o Prometheus.</p> <p>Antes da instala\u00e7\u00e3o criou-se uma pasta \"charms\" para baixar o charm (pacote de software para automatizar a implanta\u00e7\u00e3o, integra\u00e7\u00e3o e gerenciamento pelo Juju) do Grafana e do Prometheus do reposit\u00f3rio charm-hub.</p> <p></p> <p>Download do Grafana e do Prometheus</p> <p>Ap\u00f3s isso, realizou-se o deploy de ambos com aux\u00edlio do Juju e, para isso, foram alocadas as m\u00e1quinas 4 e 5, como \u00e9 poss\u00edvel obeservar na figura abaixo.</p> <p></p> <p>Deploy do Prometheus</p> <p></p> <p>Deploy do Grafana</p> <p>O passo seguinte foi integrar o Grafana com o Prometheus, a partir do seguinte comando.</p> juju integrate prometheus2:grafana-source grafana:grafana-source <p>Esse comando \u00e9 respons\u00e1vel por criar uma rela\u00e7\u00e3o entre as duas aplica\u00e7\u00f5es. \u00c9 importante salientar duas coisas:</p> <ul> <li>\"prometheus2:grafana-source\": \u00e9 o endpoint que o Prometheus exp\u00f5e para fornecer dados ao Grafana. Esse endpoint cont\u00e9m as informa\u00e7\u00f5es necess\u00e1rias para que o Grafana consiga se conectar ao Prometheus e buscar m\u00e9tricas.</li> <li>\"grafana:grafana-source\": \u00e9 o endpoint que o Grafana exp\u00f5e para receber dados de uma fonte externa, como o Prometheus.</li> </ul> <p></p> <p>Integra\u00e7\u00e3o do prometheus com o grafana</p> <p>Em seguida, acessou-se o dashboard do grafana. Por\u00e9m, para que isso fosse poss\u00edvel, foi necess\u00e1rio realizar um t\u00fanel na porta 8001 da m\u00e1quina main e na 3000 (porta do Grafana) na m\u00e1quina que foi alocada, nesse caso, o server5, logo, utilizou-se o comando abaixo.</p> ssh cloud@10.103.1.18 -L 8001:172.16.0.18:3000 <p>Desse modo, conseguiu-se acessar o Grafana.</p> <p></p> <p>Acesso ao grafana</p> <p>Para acessar o grafana, o usu\u00e1rio padr\u00e3o \u00e9: admin. Por\u00e9m, para pegar a senha foi necess\u00e1rio rodar o seguinte comando.</p> juju run grafana/0 get-admin-password <p></p> <p>Senha para o grafana</p> <p>Ap\u00f3s a realiza\u00e7\u00e3o do acesso, a senha foi alterada para \"admin\".</p> <p>Afim de verificar se a integra\u00e7\u00e3o foi feita corretamente, criou-se um dashboard dentro do grafana, utilizando o prometheus como source, assim como verificado nas figuras abaixo.</p> <p></p> <p></p> <p>Dashboard no grafana com source prometheus </p>"},{"location":"roteiro2/main/#tarefa-1","title":"Tarefa 1","text":"<ol> <li>De um print da tela do Dashboard do MAAS com as Maquinas e seus respectivos IPs.</li> <li>De um print de tela do comando \"juju status\" depois que o Grafana estiver \"active\". </li> <li>De um print da tela do Dashboard do Grafana com o Prometheus aparecendo como source.</li> <li>Prove (print) que voc\u00ea est\u00e1 conseguindo acessar o Dashboard a partir da rede do Insper.</li> <li>De um print na tela que mostra as aplica\u00e7\u00f5es sendo gerenciadas pelo JUJU.</li> </ol> <p>I.</p> <p></p> <p>II.</p> <p></p> <p>III.</p> <p></p> <p></p> <p>IV.</p> <p></p> <p>V.</p> <p></p>"},{"location":"roteiro2/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Durante este segundo roteiro, enfrentamos diversos desafios Uma das maiores dificuldades foi entender e fazer funcionar o modelo controller do Juju. Configurar corretamente a intera\u00e7\u00e3o entre o Juju e o MAAS exigiu diversas tentativas para ajustar a defini\u00e7\u00e3o da cloud e a adi\u00e7\u00e3o de credenciais. Al\u00e9m disso, a cria\u00e7\u00e3o do controlador no server1, com as devidas tags e restri\u00e7\u00f5es, mostrou-se um processo delicado, que exigiu uma verifica\u00e7\u00e3o atenta para que funcionasse. O deploy do dashboard, bem como a integra\u00e7\u00e3o entre o Grafana e o Prometheus, tamb\u00e9m apresentou seus desafios, principalmente na cria\u00e7\u00e3o de t\u00faneis e na configura\u00e7\u00e3o dos endpoints para que a comunica\u00e7\u00e3o entre as aplica\u00e7\u00f5es ocorresse sem falhas.</p>"},{"location":"roteiro2/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Em resumo, este roteiro nos permitiu ampliar o entendimento sobre a implanta\u00e7\u00e3o e gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas utilizando o Juju. A experi\u00eancia pr\u00e1tica com a cria\u00e7\u00e3o e configura\u00e7\u00e3o do controller, o deploy de aplica\u00e7\u00f5es e a integra\u00e7\u00e3o de servi\u00e7os refor\u00e7ou a import\u00e2ncia de uma infraestrutura bem planejada e executada para garantir a escalabilidade e o monitoramento eficiente do ambiente.</p>"}]}